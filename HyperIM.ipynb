{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import geoopt as gt\n",
    "\n",
    "from util import train, evalu, data\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape torch.Size([50, 120]) y_train shape torch.Size([50, 103])\n",
      "train_batch_num 40\n",
      "X_test shape torch.Size([50, 120]) y_test shape torch.Size([50, 103])\n",
      "test_batch_num 4\n",
      "train/test split 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "if(d_ball > 1):\n",
    "    from net.HyperIMxd import HyperIM\n",
    "    \n",
    "    # use pre-trained embed if avalible\n",
    "    word_embed = th.Tensor(vocab_size, embed_dim, d_ball)\n",
    "    label_embed = th.Tensor(label_num, embed_dim, d_ball)\n",
    "    \n",
    "    net = HyperIM(word_num, word_embed, label_embed, d_ball, hidden_size=embed_dim, if_gru=if_gru)\n",
    "else:\n",
    "    from net.HyperIM import HyperIM\n",
    "    \n",
    "    # use pre-trained embed if avalible    \n",
    "    word_embed = th.Tensor(vocab_size, embed_dim)\n",
    "    label_embed = th.Tensor(label_num, embed_dim)\n",
    "    \n",
    "    net = HyperIM(word_num, word_embed, label_embed, hidden_size=embed_dim, if_gru=if_gru)\n",
    "    \n",
    "    \n",
    "net.to(cuda_device)\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# optim = gt.optim.RiemannianSGD(net.parameters(), lr=lr, momentum=0.9, stabilize=1)\n",
    "optim = gt.optim.RiemannianAdam(net.parameters(), lr=lr)\n",
    "\n",
    "train_data_loader, test_data_loader = data.load_data(data_path, train_batch_size, test_batch_size, word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "batch: 0it [00:00, ?it/s]\u001b[A\n",
      "batch: 1it [00:06,  6.66s/it]\u001b[A\n",
      "batch: 2it [00:13,  6.59s/it]\u001b[A\n",
      "batch: 3it [00:20,  6.78s/it]\u001b[A\n",
      "batch: 4it [00:26,  6.72s/it]\u001b[A\n",
      "batch: 5it [00:34,  6.93s/it]\u001b[A\n",
      "batch: 6it [00:41,  7.11s/it]\u001b[A\n",
      "batch: 7it [00:49,  7.18s/it]\u001b[A\n",
      "batch: 8it [00:55,  7.05s/it]\u001b[A\n",
      "batch: 9it [01:02,  6.96s/it]\u001b[A\n",
      "batch: 10it [01:08,  6.74s/it]\u001b[A\n",
      "batch: 11it [01:14,  6.48s/it]\u001b[A\n",
      "batch: 12it [01:20,  6.38s/it]\u001b[A\n",
      "batch: 13it [01:27,  6.37s/it]\u001b[A\n",
      "batch: 14it [01:33,  6.35s/it]\u001b[A\n",
      "batch: 15it [01:40,  6.40s/it]\u001b[A\n",
      "batch: 16it [01:46,  6.46s/it]\u001b[A\n",
      "batch: 17it [01:53,  6.49s/it]\u001b[A\n",
      "batch: 18it [01:59,  6.48s/it]\u001b[A\n",
      "batch: 19it [02:06,  6.57s/it]\u001b[A\n",
      "batch: 20it [02:12,  6.41s/it]\u001b[A\n",
      "batch: 21it [02:19,  6.48s/it]\u001b[A\n",
      "batch: 22it [02:25,  6.42s/it]\u001b[A\n",
      "batch: 23it [02:31,  6.40s/it]\u001b[A\n",
      "batch: 24it [02:38,  6.39s/it]\u001b[A\n",
      "batch: 25it [02:45,  6.64s/it]\u001b[A\n",
      "batch: 26it [02:51,  6.62s/it]\u001b[A\n",
      "batch: 27it [02:58,  6.59s/it]\u001b[A\n",
      "batch: 28it [03:04,  6.45s/it]\u001b[A\n",
      "batch: 29it [03:11,  6.45s/it]\u001b[A\n",
      "batch: 30it [03:17,  6.35s/it]\u001b[A\n",
      "batch: 31it [03:23,  6.44s/it]\u001b[A\n",
      "batch: 32it [03:30,  6.63s/it]\u001b[A\n",
      "batch: 33it [03:37,  6.69s/it]\u001b[A\n",
      "batch: 34it [03:44,  6.68s/it]\u001b[A\n",
      "batch: 35it [03:50,  6.45s/it]\u001b[A\n",
      "batch: 36it [03:56,  6.35s/it]\u001b[A\n",
      "batch: 37it [04:02,  6.31s/it]\u001b[A\n",
      "batch: 38it [04:08,  6.31s/it]\u001b[A\n",
      "batch: 39it [04:15,  6.27s/it]\u001b[A\n",
      "train epoch: 100%|██████████| 1/1 [04:21<00:00, 261.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "P@1\t41.800\t\tP@3\t22.250\t\tP@5\t17.490\n",
      "nDCG@1\t41.800\t\tnDCG@3\t27.762\t\tnDCG@5\t29.216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train.train_HyperIM(epoch, net, train_data_loader, loss, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating: 4it [00:07,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1\t49.000\t\tP@3\t35.500\t\tP@5\t24.600\n",
      "nDCG@1\t49.000\t\tnDCG@3\t42.294\t\tnDCG@5\t42.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evalu.evaluate(net, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
